{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Approximate Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly find nearest neighbors in (very) high dimensions.\n",
    "\n",
    "Examples:\n",
    " * Image search and image completion\n",
    " * Song search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance functions\n",
    "\n",
    " * $d : S \\times S \\rightarrow \\mathbb{R}$ is a **distance function** iff\n",
    "     - $\\forall s, t \\in S : d(s, t) \\ge 0$\n",
    "     - $\\forall s : d(s, s) = 0$\n",
    "     - $\\forall s, t \\in S : d(s, t) = d(t, s)$\n",
    "     - $\\forall s, t, r \\in S: d(s, t) + d(t, r) \\ge d(s, r)$ (triangle inequality)\n",
    "     - if $\\forall s, t \\in S: d(s, t) = 0 \\implies s = t$, then d is a \"stronger\" function called a **metric**\n",
    " * We make use of this by representing **objects as vectors**\n",
    "     - images become feature vectors (see Computer Vision course)\n",
    "     - documents become bag-of-words or tf-idf representations\n",
    " * Many types of distances\n",
    "     - $\\ell_p$, such as the Euclidean distance ($\\ell_2$)\n",
    "     - cosine distance (used a lot in text search)\n",
    "     - edit distance (expensive)\n",
    "     - Jaccard-distance (for sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of dimensionality\n",
    "In very large dimensions, the minimum distance between any two points gets very close to the maximum distance between any points.\n",
    "\n",
    "$ \\lim_{D \\rightarrow \\infty} P[d_{max} \\le (1 + \\epsilon)d_{min}] = 1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate retrieval\n",
    "### Input\n",
    "A data set $S$ and a distance function $d$.\n",
    "\n",
    "### Problem 1: Nearest neighbor\n",
    "Given $q$, find $s* = \\text{argmin}_{s \\in S} d(q, s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Near-duplicate detection\n",
    "Find all $s$, $s'$ in $S$, with distance at most $\\epsilon$.\n",
    "\n",
    "* Use **shingling** and **Jaccard distance** as a similarity measure.\n",
    "* Can even hash shingles to save space\n",
    "* Jaccard similarity: $JSim(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "* Jaccard distance: $d(A, B) = 1 - JSim(A, B)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.33\n",
      "Distance:   0.67\n"
     ]
    }
   ],
   "source": [
    "def jaccard_sim(a, b):\n",
    "    return len(a & b) * 1.0 / len(a | b)\n",
    "\n",
    "def jaccard_distance(a, b):\n",
    "    return 1 - jaccard_sim(a, b)\n",
    "\n",
    "x = {1, 5, 6, 10}\n",
    "y = {2, 5, 6, 20}\n",
    "print(\"Similarity: %.2f\" % jaccard_sim(x, y))\n",
    "print(\"Distance:   %.2f\" % jaccard_distance(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scale remains problematic; we can't just do a double loop over all $N$ elements...\n",
    "* Hashing works well for exact duplicates, can it work with near duplicates?\n",
    "* **Yes**, we have **locality sensitive hashing** (LSH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-hashing\n",
    " * Reorder shingle matrix rows with random permutation $\\pi$\n",
    " * $\\operatorname{hash}(C) =$ minimum row number in which permuted column contains a one (C represents a column, i.e. a document in shingle form)\n",
    " * $h(C) = h_\\pi(C) = \\underset{i:C(i)=1}{\\min}\\pi(i)$\n",
    " * Turns out that the probability of two documents sharing a hash is equal to their Jaccard similarity: $P[h(C_1) = h(C_2)] = Sim(C_1, C_2)$ (trivial but interesting proof; see slides)\n",
    " * This means we can use many hash functions, see how often they clash for a pair of documents, and we have a decent estimate for the documents' similarity\n",
    " * An alternative to min-hashing is sim-hashing (see Information Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shingle_matrix = np.array([\n",
    "    [1, 0, 1, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [1, 0, 1, 0],\n",
    "    [1, 0, 1, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Want to find all duplicates with > 90% similarity\n",
    "* Apply min-hash to all documents and look for candidate pairs (documents hashed to same bucket)\n",
    "    - we find 90% of 90%-duplicates (since the probability of a single hash function colliding is 90% by design)\n",
    "    - we miss 10% of 90%-duplicates\n",
    "    - $P(\\text{miss with 1 function }) = 1 - s$ ($s$ = similarity)\n",
    "    - $P(\\text{miss with k functions}) = (1 - s)^k$\n",
    "    - multiple hash functions $\\implies$ exponentially fewer misses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def min_hash(matrix, permutation):\n",
    "    signature = [None for col in matrix[0]]\n",
    "    for index, row in enumerate(permutation):\n",
    "        for col, byte in enumerate(signature):\n",
    "            if byte is None and matrix[row][col] == 1:\n",
    "                signature[col] = index\n",
    "                    \n",
    "    return signature\n",
    "\n",
    "\n",
    "def array_sim(a1, a2):\n",
    "    in_common = len(a1[a1 == a2])\n",
    "    total = len(a1)\n",
    "#     print(\"In common: %d\" % in_common)\n",
    "#     print(\"Total:     %d\" % total)\n",
    "    return in_common * 1.0 / total\n",
    "\n",
    "\n",
    "def col_col_sim(matrix, c1_index, c2_index):\n",
    "    tr = matrix.T\n",
    "    c1 = tr[c1_index]\n",
    "    c2 = tr[c2_index]\n",
    "    return array_sim(c1, c2)\n",
    "\n",
    "    \n",
    "def sig_sig_sim(matrix, c1_index, c2_index, hash_fns):\n",
    "    sig_matrix = np.array([min_hash(matrix, hf) for hf in hash_fns])\n",
    "    tr = sig_matrix.T\n",
    "    c1 = tr[c1_index]\n",
    "    c2 = tr[c2_index]\n",
    "    return array_sim(c1, c2)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1]\n",
      "[1, 0, 3, 0]\n",
      "[1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Same permutations as in slides, just 0-indexed and reversed\n",
    "p1 = [0, 4, 1, 6, 5, 3, 2]\n",
    "p2 = [2, 1, 3, 0, 6, 4, 5]\n",
    "p3 = [4, 5, 0, 1, 6, 3, 2]\n",
    "print(min_hash(shingle_matrix, p1))\n",
    "print(min_hash(shingle_matrix, p2))\n",
    "print(min_hash(shingle_matrix, p3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns 1 and 3:\n",
      "Col/col similarity: 0.86\n",
      "Sig/sig similarity: 0.67\n",
      "Columns 2 and 4:\n",
      "Col/col similarity: 0.86\n",
      "Sig/sig similarity: 1.00\n",
      "Columns 1 and 2:\n",
      "Col/col similarity: 0.00\n",
      "Sig/sig similarity: 0.00\n",
      "Columns 3 and 4:\n",
      "Col/col similarity: 0.00\n",
      "Sig/sig similarity: 0.00\n"
     ]
    }
   ],
   "source": [
    "hash_fns = [p1, p2, p3]\n",
    "pairs = [(0, 2), (1, 3), (0, 1), (2, 3)]\n",
    "for c1, c2 in pairs:\n",
    "    print(\"Columns %d and %d:\" % (c1 + 1, c2 + 1))\n",
    "    print(\"Col/col similarity: %.2f\" % col_col_sim(shingle_matrix, c1, c2))\n",
    "    print(\"Sig/sig similarity: %.2f\" % sig_sig_sim(shingle_matrix, c1, c2, hash_fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling up min-hash\n",
    " * While the above method wok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
