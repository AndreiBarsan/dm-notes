{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Bandit Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommender systems** are a subclass of information filtering system that seek to predict the 'rating' or 'preference' that a user would give to an item.\n",
    "\n",
    "**k-armed bandits** are one way to solve this recommendation problem. They can also be used in other similar contexts, such as clinical trials and (financial) portfolio optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General concepts\n",
    "\n",
    "The **(instantaneous) regret** is the difference between the $\\mu_i$ of the arm we pick, versus the $\\mu^{*}$ of the best arm.\n",
    "\n",
    "The **total regret** is the sum of the instantaneous regrets over time. $i_1, \\dots, i_T$ are the decisions we made over time.\n",
    "\n",
    "\\begin{equation}\n",
    "R_T = \\sum_{t = 1}^T r_t = \\sum_{t = 1}^T \\mu^{*} - \\mu_{i_t}\n",
    "\\end{equation}\n",
    "\n",
    "Ideally, we pick the perfect arm from the beginning, and we get a total regret of 0.\n",
    "\n",
    "In practice, if the regret goes to zero over time, we're happy enough, and the algorithm with this propery is called **no regret**, even though there might be a little bit of regret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic k-armed bandits\n",
    "\n",
    "Like in online supervised learning, the algorithm runs over time, and at every time point we need to make a decision.\n",
    "\n",
    "k arms to pull, each arm can win ($y = $ reward $ = 1$) with probability $\\mu_i$ (unknown). Note that the reward is always constant, it's just the probability which varies.\n",
    "\n",
    "However, unlike online learning, we only receive information about the action we choose, i.e. we only have one single \"try\".\n",
    "\n",
    "In e.g. OCP, at any time point we get a new $x$, and we can try a plethora of different ways to update our model ($w$) (hypothetically) and see how good the new potential model is. With bandits, you don't know how good your choice is until you commit to it and do it (pull the arm), by which time, you can no longer change it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-greedy algorithm\n",
    "\n",
    "At every time, pick random arm with probability $\\epsilon$, and pick the current best arm otherwise.\n",
    "\n",
    "This works surprisingly well (it's no-regret), but could be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoeffding's inequality\n",
    "> [...] provides an upper bound on the probability that the sum of random variables deviates from its expected value. \n",
    ">\n",
    "> -- Wikipedia\n",
    "\n",
    "Let $X_1,\\dots,X_m$ be i.i.d. random variables taking values in $[0, 1]$.\n",
    "\n",
    "The real mean $\\mu = \\mathbb{E}[X]$ is unknown.\n",
    "\n",
    "We have an empirical estimate based on our $m$ trials:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\mu}_m = \\frac{1}{m} \\sum_{i = 1}^{m} X_i\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have:\n",
    "\n",
    "\\begin{equation}\n",
    "    P\\left(\\left|\\mu - \\hat{\\mu}_m\\right| \\ge b\\right) \\le 2 \\exp\\left(-2b^2m \\right) = \\delta_m\n",
    "\\end{equation}\n",
    "\n",
    "That is, the probability of our operation being more than $b$ off the real value is smaller than a computable threshold.\n",
    "\n",
    "We just want a lower bound $b$ for any given fixed probability bound. So we fix the probability bound to, say, $\\delta_m$, and then compute the corresponding $b$, as a function of $\\delta_m$ and $m$.\n",
    "\n",
    "For a fixed upper probability bound, we fix $\\delta_m$ and get $b = \\sqrt{\\frac{1}{2m} \\log{\\frac{2}{\\delta_m}}}$.\n",
    "\n",
    "All we need now is to decide what $\\delta_m$ should be.\n",
    "\n",
    "Now we also want to set an upper bound on the probability not just for a single $m$, but for all $m$. Want upper bound for $E_m = P(|\\mu - \\hat{\\mu}_m| \\ge b)$, i.e. a lower bound for $P(|\\mu - \\hat{\\mu}_t| \\ge b, \\> \\forall t)$.\n",
    "\n",
    "So we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    P(|\\mu - \\hat{\\mu}_t| \\le b, \\> \\forall t) & = 1 - P(E_1 \\cup E_2 \\cup \\dots) \\\\\n",
    "    & \\ge 1 - \\sum_{t=1}^{\\infty} P(E_t) \\\\\n",
    "    & \\ge 1 - \\sum_{t=1}^{\\infty} \\delta_t \\\\\n",
    "    & \\ge 1 - \\delta \\> \\text{with} \\> \\delta < \\sum_{t=1}^{\\infty}\\delta_t\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "2nd row smaller since the sum we're subtracting is bigger (longer than the prev.).\n",
    "3rd row smaller because all deltas are larger than their corresponding Es, as defined by Hoeffding's inequality itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore want a sum of $\\delta_t$ which is bounded. Setting $\\delta_t = \\frac{c}{t^2}$ works well, since the correspoinding sum converges, so the upper bound $\\delta$ exists and is finite.\n",
    "\n",
    "We now have a good heuristic: at any time step $t$, our upper bound should be $\\delta_t = \\frac{c}{t^2}$.\n",
    "\n",
    "Recall that we want to express $b$ as a function of $\\delta_t$, since $b$ is *the value which actually defines our upper confidence bound*.\n",
    "\n",
    "(Note that this probability shrinks quadratically over time, so it keeps getting tighter and tighter for all arms, as we keep playing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCB1\n",
    "\n",
    "All we need to do now is shove our $\\delta_t$ in the formula for $b$, and we get (setting $c := 2$):\n",
    "\n",
    "\\begin{aligned}\n",
    "\\operatorname{UCB}(i) \n",
    "& = \\hat{\\mu}_i + \\sqrt{\\frac{1}{n_i} \\ln (2  \\frac{t^2}{2}) } \\\\\n",
    "& = \\hat{\\mu}_i + \\sqrt{\\frac{\\ln{t^2}}{n_i}} \\\\\n",
    "& = \\hat{\\mu}_i + \\sqrt{\\frac{2 \\ln{t}}{n_i}}\n",
    "\\end{aligned}\n",
    "\n",
    "We can plug this formula right into a program! See `bonus/tutorial-bandits.ipynb` for an implementation.\n",
    "\n",
    "This is an algorithm which is much smarter than $\\epsilon$-greedy about what it explores.\n",
    "\n",
    "TODO: there seems to be a \"2\" missing somewhere. Low priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that UCB is a **no-regret** algorithm. ($R_T / T \\rightarrow 0 \\> \\text{as} \\> T \\rightarrow \\infty$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of bandit algorithms\n",
    "Non-DM:\n",
    " * Clinical trials (give the best possible cure to a patient, while also working on improving the accuracy of our diagnostics)\n",
    " * Matching markets (TODO: more info)\n",
    " * Asset pricing\n",
    " * Adaptive routing\n",
    " * Go\n",
    " \n",
    "DM:\n",
    " * Advertising\n",
    " * Optimizing relevance (e.g. news article recommendations)\n",
    " * Scheduling web crawlers\n",
    " * Optimizing user interfaces (e.g. smart A/B testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual bandits\n",
    "Also incorporate some info about every arm and every user. Useful when e.g. recommending articles, since it takes users topic preferences into account.\n",
    "\n",
    "We still use **cummulative (contextual) regret** as a metric, $R_t = \\sum_{t=1}^{T}r_t$.\n",
    "\n",
    "Can achieve *sublinear regret* by learning **optimal mapping** from contexts (e.g. (user, article features) tuples) to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "* Observe context: $z_t \\in \\mathcal{Z}$, and, e.g. $\\mathcal{Z} \\subseteq \\mathbb{R}^{d}$\n",
    " * Pick arm from set of possible arms, $x_t \\in \\mathcal{A}_t$\n",
    " * Observe reward $y_t$, which depends on the picked arm and the context, plus some possible noise: $y_t = f(x_t, z_t) + \\epsilon_t$\n",
    " * Incur regret: $r_t = \\max_{x \\in \\mathcal{A}_t}(f(x, z_t)) - f(x_t, z_t)$ (like before, difference between the best arm, given the context, and the arm we actually picked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear recommendations\n",
    "\n",
    "Want to minimize regularized square loss for\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{w}_i = \\arg \\min_w \\sum_{t=1}^{m} (y_t - w^T z_t)^2 + \\|w\\|_2^2\n",
    "\\end{equation}\n",
    "\n",
    "Note: This model can take features from the user, the article, or both  into account. And every article has its own $\\hat{w}$. This is linear regression and it's easy to solve.\n",
    "\n",
    "Key idea: Want to merge UCB and regression by having an upper confidence bound (UCB) for our $w$s. Ideally, just as in UCB1, this bound will shrink towards $w$ as time goes on.\n",
    "\n",
    "This is LinUCB. [CHEATSHEET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    " \\left| \\> \\text{estimated reward} - \\text{true reward} \\> \\right| & \\le \\text{some bound} \\quad \\text{(with some probability)} \\\\\n",
    " \\left| \\hat{w}^T_i z_t - w^T_i z_t \\right| & \n",
    " \\le \\alpha\\sqrt{z^T_t(D^T_i D_i + I)^{-1}z_t}, \\> p \\ge 1 - \\delta \\\\\n",
    " \\left| \\hat{w}^T_i z_t - w^T_i z_t \\right| & \n",
    " \\le \\alpha\\sqrt{z^T_t M_i z_t}, \\> p \\ge 1 - \\delta\n",
    "\\end{aligned}\n",
    "\n",
    "This holds as long as $\\alpha = 1 + \\sqrt{\\frac{1}{2} \\ln \\left( \\frac{2}{\\delta} \\right)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set our desired probability bound, compute $\\alpha$ and we have an algorithm!\n",
    "\n",
    "Same as UCB1, but compute an arm's UCB as:\n",
    "\n",
    "\\begin{aligned}\n",
    "    M_x \\in \\mathbb{R}^{d \\times d}, b_x \\in \\mathbb{R}^{d} & \\quad \\text{(the arm's model)} \\\\\n",
    "    \\hat{w} = M_x^{-1} b & \\quad \\text{(the model used for the primary payoff prediction)} \\\\\n",
    "    \\operatorname{UCB}_x = \\hat{w}_x^T z_t + \\alpha \\sqrt{z_t^T M_t^{-1} z_t} & \\quad \\text{(arm UCB given z)}\n",
    "\\end{aligned}\n",
    "\n",
    "Not storing $M_x$ and $b_x$ together because we need $M_x^{-1}$ to compute the upper confidence bound of our predicted payoff.\n",
    "\n",
    "LinUCB is also no-regret (i.e. regret sub-linear in T)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from $y_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem with linear recommendations\n",
    "\n",
    "No shared effect modeling. We optimize every arm separately based on what users like it, but there's no way to directly exploit the fact that similar users may like similar articles.\n",
    "\n",
    "Use hybrid models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid LinUCB\n",
    "\n",
    "\\begin{equation}\n",
    "    y_t = w_i^T z_t + \\beta^T \\phi(x_i, z_t) + \\epsilon_t\n",
    "\\end{equation}\n",
    "\n",
    " * $\\phi(x, z)$ simply flattens (like `numpy.ravel`) $x_i z_t^T$.\n",
    " * $w_i$ is an arm's model\n",
    " * $\\beta$ captures user-article similarity (i.e. user interests).\n",
    " \n",
    "Can also solve this using regularized regression. We also need to compute confidence intervals for this bad boy. The algorithm is fluffy, but it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical implementation of contextual bandits\n",
    "\n",
    "Sample case:\n",
    " * 1193 user features, 81 article features\n",
    " * We need to perform dimensionality reduction!\n",
    " \n",
    "### Extracting feature vectors\n",
    " * Data consists of triplets of form (article_features, user_features, reward): $D = \\left\\{ (\\phi_{a,1}, \\phi_{u,1}, y_1), \\dots, (\\phi_{a,n}, \\phi_{u,n}, y_n) \\right\\}$\n",
    " * Learn the model parameters $W$ with logistic regression (remember that our reward is 1 or 0, e.g. click or no click).\n",
    " * Set: $\\psi_{a,i} = \\phi^T_{a,i} W$ (vector) (TODO: more details)\n",
    " * k-means cluster $\\psi_{a, i}$ (i.e. over i datapoints, with $i = 1, \\dots, n$)\n",
    " * Obtain $j < n$ clusters; the final article features for article $i$ are $x_{i, j} = \\frac{1}{Z} \\exp{\\left( -\\| \\psi_{a, i} - \\mu_j \\|_2^2 \\right)}, \\> x_{i, j} \\in \\mathbb{R}^{k}$\n",
    " * I.e. compute some clusters and model articles relative to them, i.e. every article's feature is its distance to that cluster (and exp + constant, but the principle stays the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating bandit algorithms\n",
    "\n",
    "Gather data with pure exploration (random).\n",
    "\n",
    "Learn from log using **rejection sampling**. Go through log and try to predict the click at every step. If we're wrong, reject the sample (ignore log line), if we're right, feed back that reward into the algorithm.\n",
    "\n",
    "Stop when T events kept.\n",
    "\n",
    "This is what we did in the last project!\n",
    "\n",
    "This approach is **unbiased**, and the expected number of needed events are $kT$, with $k$ being the (post-dim-red) number of article features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: talk about the comparison, and how different algorithms behave with respect to e.g. arm count, data sie, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing observations across users\n",
    " * Use stereotypes\n",
    " * Describe stereotypes in lower-dim space (estimate using PCA/SVD, so dim-reduction).\n",
    " * First explore in stereotype subspace, then in the full space (whose exploration is significantly more expensive). This is **coarse to fine bandits**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets of k recommendations\n",
    " * Want to choose set that's relevant to as many users as possible.\n",
    " * $\\implies$ optimize **relevance** and **diversity**\n",
    " * Want to cover as many users as possible with a (limited) set of e.g. ads.\n",
    " * Every article $i$ is relevant to a set of users $S_i$. Suppose this is known.\n",
    " * Define the coverage of a set $A$ of articles:\n",
    " \n",
    "\\begin{equation}\n",
    "F(A) = \\left| \\bigcup_{i \\in A}S_i \\right|\n",
    "\\end{equation}\n",
    "\n",
    " * And we want to maximize this coverage: $\\max_{|A| \\le k} F(A)$\n",
    "     - nr. sets A grows exponentially in $k$\n",
    "     - finding the optimal A is NP-hard.\n",
    "     - Let's try a greedy solution!\n",
    "     - Start with $A_0$ empty, and always add the article which increases the coverage of $A$ the most.\n",
    "     - Turns out, this solution is \"good enough\" (~63% of optimal)\n",
    "     - $F(\\> \\text{greedy set of size} \\> l \\>) \\ge \\left(1 - e\\left( -\\frac{l}{k}\\right)\\right) \\max_{|A| \\le k}F(A)$ TODO: hard to tell from prof.'s writing; double check!\n",
    "     - this works because F is non-negative monotone and submodular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submodularity\n",
    "[EXAM] **Submodularity** is a property of *set functions*.\n",
    "\n",
    "$F : 2^V \\rightarrow \\mathbb{R} \\> \\text{submodular} \\iff \\forall A \\subseteq B, s \\not\\in B: F(A \\cup \\{s\\}) - F(A) \\ge F(B \\cup \\{s\\}) - F(B)$\n",
    "\n",
    "Adding a set earlier cannot be worse than adding it later.\n",
    "\n",
    "Marginal benefits can never increase, i.e. our delta improvement at every step only gets smaller and smaller.\n",
    "\n",
    "**Closedness**: A weighted sum of submodular functions is also submodular (positive weights). (Closed under nonnegative linear combinations.)\n",
    " * Allows multi-objective optimization with weights, as long as each objective is itself submodular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Lazy\" greedy algorithm\n",
    " * First iteration as usual.\n",
    " * Keep ordered list of marginal benefits $\\Delta_i$ for every option from previous iteration.\n",
    " * Re-evaluate $\\Delta_i$ only for top element.\n",
    " * If $\\Delta_i$ stays on top, use it; otherwise, re-sort.\n",
    " \n",
    "TODO: Explain examples. How does this actually relate to e.g. article recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Bandit submodular optimization: learn from observing **marginal gains**\n",
    " * $F_t(A_t)$ is the feedback at time $t$, given that the set of articles $A_t$ was shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
