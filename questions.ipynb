{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question repository\n",
    "\n",
    "A list of open questions and possibly ambiguous stuff encountered throughout the material.\n",
    "\n",
    "TODO: Tag exam-related ones appropriately, to differentiate them from (exclusively) curiosity-related ones.\n",
    "\n",
    "**Note:** An alternative design would consist of adding a questions section to every notebook, tagging it appropriately using IPython metadata, and then using something like a Python/shell script to print all open questions in a centralized way. However...\n",
    "\n",
    "![Ain't nobody got time fo' dat!](http://vignette1.wikia.nocookie.net/starpolar/images/6/6b/Notime.jpg/revision/latest?cb=20150225125846)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Approximate retrieval\n",
    "* Why perform first step of hashing if we only have a small number of features (e.g. 100)? If many features, why not just do a PCA first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification\n",
    " * When transitioning from the first SVM formulation (with slack variables), to the second one aren't we loosening any constraint by fixing $\\xi$?\n",
    "     * (tentative) It seems we're not, since we're taking multiple cases into consideration and merging them together into a single formulation using max.\n",
    " * Slide 04:18: Is the first (primal) SVM formulation a (ii)-type one (since it has a minimization and its constraint as separate equation), or is it not eligible for this categorization?\n",
    " * Slide 06:15: How do we go from step 1 to 2? Isn't the $\\lambda \\| w \\|_2^2$ term outside the sum?\n",
    "  - yes it is, but the sum has a convenient $\\frac{1}{T}$ in front of it, so we're safe to add the regularization term into the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Why do some SVM OCP implementations *always* regularize, even when the model was not updated at that stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Non-linear classification\n",
    " * How exactly is the Lagrangian dual reformulation step (SVMs) different from the first time we reformulated the SVM problem statement to get rid of the slack variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active learning\n",
    " * When doing active learning based on uncertainty sampling, how exactly do we know when we can safely infer some labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering\n",
    " * Homework 5 solution, 2.2: Why is:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\operatorname{Var}_{\\hat{x}_i \\sim q}\\left [ \\frac{1}{m} \\sum_{i=1}^{m} \\frac{d(\\hat{x}_i; \\mu)}{q(\\hat{x}_i)} \\right ] = \\frac{1}{m^2} \\sum_{i=1}^m \\operatorname{Var}_{x_i \\sim q} \\left[ \\frac{d(x_i; \\mu)}{q(x_i)} \\right]\n",
    "\\end{equation}\n",
    "\n",
    " * And why doe we still have the $i$ subscript in the variance formulation? Can't we just write $x \\tilde{} q$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
