{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Assign **labels** to **data points**\n",
    " * Training examples generalized to a **decision rule** (or more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers\n",
    " * Model expressed as a normal $w$ to a hyperplane separating two classes (and, optionally, a bias (offset) term $b$).\n",
    " * Linear classifiers are easy to understand, quick to train, work very well (especially in high dimensions) and make extremely efficient predictions.\n",
    " * In all future examples, use *homgeneous representation* and only define a model as $w$, with no bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    " * Max-margin classifiers\n",
    " * Find $w$ so that the margin to closest representatives of each class is maximized. Can be shown (but not here) that this is the right thing to do in order to improved our generalization.\n",
    " * Margin size is $\\frac{1}{||w||_2} = \\mathit{margin}$\n",
    " * There's always noise $\\implies$ model as slack variables.\n",
    "\n",
    "### Convex optimization\n",
    " * This is the way we solve the SVM constrained minimization problem\n",
    " * Minimization because we want to \"shrink\" our model's magnitude (i.e. the margin size) and, as much as possible, the impact of the slack variables (data points which we cannot avoid misclassifying).\n",
    " * Constrained because we also want to ensure our model does the right thing and correctly classifies most points (for most points, we expect their $\\xi_i$ to be 0, and $y_iw^Tx_i\\ge1$, meaning that the point is snugly placed in the correct class; positive means the right side of the separation hyperplane; also greater than one means it's beyond the separation margin).\n",
    " * **Problem:** current approach has a run-time complexity of $\\Omega(n^2)$, and simply doesn't work if the data don't fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex optimization, reformulated\n",
    " * Because linear algebra is awesome, we can reformulate the (primal) SVM definition into something a little more useful.\n",
    " * $\\xi_i = \\operatorname{max}(0, 1 - y_iw^Tx_i)$, from the constraint part; it's called hinge loss ($l_{\\operatorname{hinge}}$)\n",
    " * Shove this into the minimization formula.\n",
    " * We get: $\\min_w{\\left (w^Tw + C\\sum\\limits_{i}\\max(0, 1 - y_iw^Tx_i) \\right )}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex optimization, reformulated, again\n",
    " * We can formulate a multicriteria optimization problem in two ways:\n",
    "     * Minimize everything at once\n",
    "     * Minimize one objective, while keeping the other under a certain bound\n",
    " * Our above (re)formulation belongs to the first category (one big-ass $\\min$)\n",
    " * Let's write it in the second way.\n",
    " * Minimize the second part, as long as the first part ($w^Tw$) is below some threshold B.\n",
    " * We write that as:\n",
    "     * $\\min_w\\sum\\limits_i\\max(0, 1 - y_i w^T x_i)$ s. t. $||w||_2 \\lt \\frac{1}{\\sqrt{\\lambda}}$\n",
    " * Interestingly enough, all 3 of our formulations produce the **same solutions**. TODO: ask algebra guru for more details about why.\n",
    " * However, the complexity varies. A lot. The last version is the best in that regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex optimization, in general\n",
    " * Many supervized learning problems consists of two components: a loss function and a regularizer.\n",
    " * From a (handwavily explained) Bayesian standpoint, the former represents the evidence, while the latter, our prior knowledge (i.e. we *know* our model shouldn't be *too* complex) [citation needed]\n",
    " * As we saw before, there are two ways to state these problems mathematically.\n",
    "     * Using a single \"large\" minimization (l + r)\n",
    "     * Using a little minimization (l) plus a constraint (r)\n",
    " * We will focus on the second techniques, as it makes it possible to perform online learning.\n",
    " \n",
    " \n",
    "OCP -> online-to-batch conversion by averaging if we want to train a single SVM on fixed data set\n",
    "If we also pick data at random -> SGD.\n",
    "\n",
    "* TODO: strong convexity and its benefits \n",
    "    * Using geometric examples can really help!\n",
    "* TODO: adapting to geometry\n",
    "    * ADAGRAD explained really well in tutorial(s)\n",
    "* TODO: parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open questions\n",
    " * When transitioning from the first SVM formulation (with slack variables), to the second one aren't we loosening any constraint by fixing $\\xi$?\n",
    "     * (tentative) It seems we're not, since we're taking multiple cases into consideration and merging them together into a single formulation using max.\n",
    " * Slide 04:18: Is the first (primal) SVM formulation a (ii)-type one (since it has a minimization and its constraint as separate equation), or is it not eligible for this categorization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
