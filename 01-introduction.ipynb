{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is data mining?\n",
    " * Semi-automatic procedures to find general and useful patterns in large data sets.\n",
    "\n",
    "## Applications\n",
    " * **Approximate retrieval**: Finding similar elements (similar songs, image search, plagiarism detection, copyright protection, etc.) in giant datasets\n",
    " * **Supervised learning**, such as large scale classification (of user behavior, of images, of text, etc.) and regression\n",
    " * **Unsupervised learning**, such as large scale clustering (search for groups of similar users, images, songs, articles, etc.) and dimension reduction\n",
    " * **Recommender systems** (bandit algorithms ($\\epsilon$-greedy, UCB1, LinUCB, Hybrid LinUCB, etc.) and their applications in fields such as news article recommendation and adverting)\n",
    " * Others (monitoring transients in astronomy, spam filtering, fraud detection, machine translation, six degrees of ~~Kevin Bacon~~ separation etc.)\n",
    " \n",
    " \n",
    "## Scale\n",
    " * Example: 10-100 TB of data per sky survey (astronomy)\n",
    " * Archive sizes measured in **petabytes**\n",
    " * Real-time data flows (e.g. computing trends in social network)\n",
    " * Data sources\n",
    "     - science\n",
    "     - commercial/civil/engineering\n",
    "     - security/intelligence/defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical aspects\n",
    " * Want to keep data in main memory as much as possible (faster)\n",
    " * If data don't fir in the main memory, we have to access it in a streaming fashion. Random access would be much too expensive, so we have to adapt our algorithms in order to learn from streaming data.\n",
    " * Want *real-time analytics*\n",
    " * Want real-time synthesis\n",
    " * Want to leverage large-scale parallelism (across entire data centers)\n",
    " * Data quality often sucks (missing elements, missing elements represented as seemingly-present elements (null vs. \"\" vs. 0 vs. \"\\0\" vs. undefined, etc.), inconsistent schema, etc.)\n",
    " * Need to respect users' privacy (control direct access to data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not covered\n",
    " * Systems issues (databases, data center management, etc.)\n",
    " * Specialized data structures\n",
    " * Domain specific algorithms\n",
    "     - see **Information retrieval** course for more text-specific elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    " * Works well with commodity hardware in data centers (DCs)\n",
    " * Failure-tolerant (redundancy over DC)\n",
    " * Works with distributed file systems (e.g. Google GFS, HDFS, etc.), which are optimized for durability, frequent reads and appends, but rare updates\n",
    " * `map(key, value)` and `reduce(key, values)` (bread and butter; other operations exist); the default shuffler does a lot of the grunt work!\n",
    " * **Partitions** the input data, **schedules** program execution across a set of machines, handles machine **failures**, and manages inter-machine **communication**\n",
    " * A job's output is often another job's input; many tools support multi-stage pipelines natively (Cascading, Scalding, Spark, MS DryadLINQ, etc.)\n",
    " * Stragglers (last few remaining reducers) $\\implies$ spawn multiple copies of job and take the result of whoever finishes first\n",
    " * Hadoop is the most common MapReduce implementation; relies a lot on disk access $\\implies$ slow; Spark offers massive speedups by relying less on disk access\n",
    "\n",
    "Trick to compute variance in one pass: use formula based on expectation ($\\mathbb{V}ar(X) = \\hat{\\mathbb{E}}[X^2] - \\hat{\\mathbb{E}}[X]^2$).\n",
    "\n",
    "GPGPUs can also offer massive speed-ups when used right. They are not covered in this course, but are very widely used for algorithms requiring heavy number-crunching (many vector/matrix operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
